{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP Assignment 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/srinath68/NLP/blob/main/NLP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsvnhG2nYWMW"
      },
      "source": [
        "initialising of NLTK and tokenization and downloading required packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx5g_HX0V6uL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2a6ce4-343c-4d2f-db99-7f8241792de2"
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "npsLPT6VYtXR"
      },
      "source": [
        "Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.\n",
        "\n",
        "Split the above paragraph into \n",
        "\n",
        "Q1.sentences and \n",
        "\n",
        "Q2.words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyTiawYrWvdb"
      },
      "source": [
        "para=\"Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?  Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin? Machines, after all, recognize numbers, not the letters of our language. And that can be a tricky landscape to navigate in machine learning.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKqa-i-rZG7W"
      },
      "source": [
        "splitting into sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLZi7TLUW45y",
        "outputId": "e342d8e6-95fc-4077-c421-524a2650701d"
      },
      "source": [
        "sen=nltk.sent_tokenize(para)\n",
        "sen"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are  you  fascinated  by  the  amount  of  text  data  available  on  the  internet?',\n",
              " 'Are  you looking  for  ways  to  work  with  this  text  data  but  aren’t  sure  where  to  begin?',\n",
              " 'Machines, after all, recognize numbers, not the letters of our language.',\n",
              " 'And that can be a tricky landscape to navigate in machine learning.']"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsaWhTL8b196"
      },
      "source": [
        "splitting into words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EF-9K--vXuyo",
        "outputId": "6b72201d-aac0-41bb-d8a7-8764adf70b57"
      },
      "source": [
        "words=word_tokenize(para)\n",
        "print(words)\n",
        "\n",
        "print(\"\\n\\n\")\n",
        "words\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Are', 'you', 'fascinated', 'by', 'the', 'amount', 'of', 'text', 'data', 'available', 'on', 'the', 'internet', '?', 'Are', 'you', 'looking', 'for', 'ways', 'to', 'work', 'with', 'this', 'text', 'data', 'but', 'aren', '’', 't', 'sure', 'where', 'to', 'begin', '?', 'Machines', ',', 'after', 'all', ',', 'recognize', 'numbers', ',', 'not', 'the', 'letters', 'of', 'our', 'language', '.', 'And', 'that', 'can', 'be', 'a', 'tricky', 'landscape', 'to', 'navigate', 'in', 'machine', 'learning', '.']\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Are',\n",
              " 'you',\n",
              " 'fascinated',\n",
              " 'by',\n",
              " 'the',\n",
              " 'amount',\n",
              " 'of',\n",
              " 'text',\n",
              " 'data',\n",
              " 'available',\n",
              " 'on',\n",
              " 'the',\n",
              " 'internet',\n",
              " '?',\n",
              " 'Are',\n",
              " 'you',\n",
              " 'looking',\n",
              " 'for',\n",
              " 'ways',\n",
              " 'to',\n",
              " 'work',\n",
              " 'with',\n",
              " 'this',\n",
              " 'text',\n",
              " 'data',\n",
              " 'but',\n",
              " 'aren',\n",
              " '’',\n",
              " 't',\n",
              " 'sure',\n",
              " 'where',\n",
              " 'to',\n",
              " 'begin',\n",
              " '?',\n",
              " 'Machines',\n",
              " ',',\n",
              " 'after',\n",
              " 'all',\n",
              " ',',\n",
              " 'recognize',\n",
              " 'numbers',\n",
              " ',',\n",
              " 'not',\n",
              " 'the',\n",
              " 'letters',\n",
              " 'of',\n",
              " 'our',\n",
              " 'language',\n",
              " '.',\n",
              " 'And',\n",
              " 'that',\n",
              " 'can',\n",
              " 'be',\n",
              " 'a',\n",
              " 'tricky',\n",
              " 'landscape',\n",
              " 'to',\n",
              " 'navigate',\n",
              " 'in',\n",
              " 'machine',\n",
              " 'learning',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fRnnTLBpcH76"
      },
      "source": [
        "Q3. Find (i)stem and (ii)lemma words for the given words?\n",
        "\n",
        "\"cats\"\n",
        "\n",
        "\"trouble\"\n",
        "\n",
        "\"troubling\"\n",
        "\n",
        "\"troubled\"\n",
        "\n",
        "“having”\n",
        "\n",
        "“Corriendo”\n",
        "\n",
        "“at”\n",
        "\n",
        "“was”"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mf3NKErBUDLk"
      },
      "source": [
        "lists=[\"cats\",\"trouble\",\"troubling\",\"troubled\",\"having\",\"Corriendo\",\"at\",\"was\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeDtWhlHcawD"
      },
      "source": [
        "(i) stem words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coo8A3aTSPX_",
        "outputId": "9939592d-d917-4566-8dbc-735846fd3e83"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "ps = PorterStemmer()\n",
        "for word in lists:\n",
        "\tstem_rootWord=ps.stem(word)\n",
        "\tprint(stem_rootWord)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "troubl\n",
            "troubl\n",
            "troubl\n",
            "have\n",
            "corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmZQgo_5c8P4"
      },
      "source": [
        "(ii) lemma words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xLXNL5h-UQA4",
        "outputId": "d06601d0-0ab6-4580-d161-2520ce4fa4b2"
      },
      "source": [
        "from nltk.stem import \tWordNetLemmatizer\n",
        "wl = WordNetLemmatizer()\n",
        "for w in lists:\n",
        "\t\tprint(wl.lemmatize(w))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cat\n",
            "trouble\n",
            "troubling\n",
            "troubled\n",
            "having\n",
            "Corriendo\n",
            "at\n",
            "wa\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJyBgMmMdIB4"
      },
      "source": [
        "Q4. Find stop words from the given paragraph?\n",
        "\n",
        "The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POFh_8ItN6vR"
      },
      "source": [
        "text=\"The NLTK library  is  one  of  the  oldest  and  most  commonly  used  Python  libraries  for Natural Language Processing. NLTK supports stop word removal, and you can find the list of stop words in the  corpus  module. To remove stop words from a sentence, you can divide your text into words and then remove the word if it exits in the list of stop words provided by NLTK.\"\n",
        "token=word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tLqMd_OEeFw0"
      },
      "source": [
        "finding stopwords "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TkL41e-Tk4j",
        "outputId": "ba87ec02-795e-4e4c-850b-9c0359d01bc4"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stops = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "ss=[]\n",
        "for w in token:\n",
        "    if w in stops:\n",
        "        ss.append(w)\n",
        "\n",
        "print(\"stopwords with duplicate words \")\n",
        "print(ss)\n",
        "\n",
        "new_stop = []\n",
        "[new_stop.append(x) for x in ss if x not in new_stop]\n",
        "\n",
        "print(\"\\nstopwords without dupication of words\")\n",
        "print(new_stop)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stopwords with duplicate words \n",
            "['is', 'of', 'the', 'and', 'most', 'for', 'and', 'you', 'can', 'the', 'of', 'in', 'the', 'from', 'a', 'you', 'can', 'your', 'into', 'and', 'then', 'the', 'if', 'it', 'in', 'the', 'of', 'by']\n",
            "\n",
            "stopwords without dupication of words\n",
            "['is', 'of', 'the', 'and', 'most', 'for', 'you', 'can', 'in', 'from', 'a', 'your', 'into', 'then', 'if', 'it', 'by']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTMuDDxVeV9i"
      },
      "source": [
        "Q5. From the above paragraph print frequency of each word using NLTK?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ObqBKDPqPr_g",
        "outputId": "55e91ef6-d8a9-4065-dd11-d2b2647dacb1"
      },
      "source": [
        "from nltk.probability import FreqDist\n",
        "\n",
        "fdist = FreqDist(token)\n",
        "\n",
        "for k in sorted(fdist):\n",
        "    print(\"%s: %s\" % (k, fdist[k]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ",: 2\n",
            ".: 3\n",
            "Language: 1\n",
            "NLTK: 3\n",
            "Natural: 1\n",
            "Processing: 1\n",
            "Python: 1\n",
            "The: 1\n",
            "To: 1\n",
            "a: 1\n",
            "and: 3\n",
            "by: 1\n",
            "can: 2\n",
            "commonly: 1\n",
            "corpus: 1\n",
            "divide: 1\n",
            "exits: 1\n",
            "find: 1\n",
            "for: 1\n",
            "from: 1\n",
            "if: 1\n",
            "in: 2\n",
            "into: 1\n",
            "is: 1\n",
            "it: 1\n",
            "libraries: 1\n",
            "library: 1\n",
            "list: 2\n",
            "module: 1\n",
            "most: 1\n",
            "of: 3\n",
            "oldest: 1\n",
            "one: 1\n",
            "provided: 1\n",
            "removal: 1\n",
            "remove: 2\n",
            "sentence: 1\n",
            "stop: 4\n",
            "supports: 1\n",
            "text: 1\n",
            "the: 5\n",
            "then: 1\n",
            "used: 1\n",
            "word: 2\n",
            "words: 4\n",
            "you: 2\n",
            "your: 1\n"
          ]
        }
      ]
    }
  ]
}